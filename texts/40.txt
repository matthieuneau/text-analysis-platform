by jason kohn, contributing columnist like many of us, scientific researchers tend to be creatures of habit. this includes research teams working for the national oceanic and atmospheric administration ( noaa ), the u. s. government agency charged with measuring the behavior of oceans, atmosphere, and weather. many of these climate scientists work with massive amounts of data – for example, the national weather service collecting up - to - the - minute temperature, humidity, and barometric readings from thousands of sites across the united states to help forecast weather. research teams then rely on some the largest, most powerful high - performance computing ( hpc ) systems in the world to run models, forecasts, and other research computations. given the reliance on hpc resources, noaa climate researchers have traditionally worked onsite at major supercomputing facilities, such as oak ridge national laboratory in tennessee, where access to supercomputers are just steps away. as researchers crate ever more sophisticated models of ocean and atmospheric behavior, however, the hpc requirements have become truly staggering. now, noaa is using a super - high - speed network called “ n - wave ” to connect research sites across the united states with the computing resources they need. the network has been operating for several years, and today transports enough data to fill a 10 - gbps network to full capacity, all day, every day. noaa is now upgrading this network to allow even more data traffic, with the goal of ultimately supporting 100 - gbps data rates. “ our scientists were really used to having a computer in their basement, ” says jerry janssen, manager, n - wave network, noaa, in a video about the project. “ when that computer moved